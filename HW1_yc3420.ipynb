{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total points for this HW: 100.\n",
    "\n",
    "Please note: Copying and pasting other people's work is absolutely prohibited.  Any such cases will be reported to CUSP's education team and severely punished. Discussion is encouraged, and feel free to exchange ideas with your classmates, but please write your own code and do your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 1: Accuracy and interpretability (10 pts)\n",
    "\n",
    "a) Describe a real-world prediction problem using urban data for which _interpretability_ of your models and results is essential, and for which it might be preferable to use decision trees rather than random forests.  Argue why this is the case. (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The decision trees could be superior to random forest when the interpretability is more important in the problem.  Also, decision trees could be shown to non-tech people to explain how we come up with the conclusion. For example: when we try to predict whether there is a traffic jam in a certain place, we could use a few features (car number, average speed from historical data, traffic light number, traffic light length) to construct a decision tree.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Describe a real-world prediction problem using urban data for which _accuracy_ is paramount and interpretability may be less important, and for which it might be preferable to use random forests rather than decision trees.  Argue why this is the case. (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The prediction of house price. People usually do not care too much about how the conclusion is came up with, for example, people usually do not care about what is the coefficient of an extra bathroom or a balcony when predicting the house price, rather, they will more focus on the final prediction results. Moreover, in this kind of prediction, there are too many features that could lead to overfitting with the decision tree method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Let's imagine that you want to try to get the best of both worlds (accuracy _and_ interpretability).  So you decide to start by learning a random forest classifier.  Describe at least one way of getting some interpretability out of the model by post-processing.  You could either pick a method from the literature (e.g., Domingos's work on combining multiple models or some method of computing variable importance), or come up with your own approach (doesn't have to be ground-breaking, but feel free to be creative!) (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we known previously, random forest is more concentrated on accuracy and it performed as a blackbox. One way of getting an insight into a random forest is to compute feature importances, either by permuting the values of each feature one by one and checking how it changes the model performance or computing the amount of “impurity” (typically variance in case of regression trees and gini coefficient or entropy in case of classification trees) each feature removes when it is used in node. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question 2: Build a decision tree for classification, step by step, following the lecture notes. Note that the dataset has been slightly modified, so you will get a different tree than the one shown in the lecture notes.  (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>HP</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>75</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>175</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>94</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>95</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>139</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>190</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>145</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bad</td>\n",
       "      <td>8</td>\n",
       "      <td>170</td>\n",
       "      <td>weighty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>89</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>81</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>95</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bad</td>\n",
       "      <td>4</td>\n",
       "      <td>93</td>\n",
       "      <td>light</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MPG  cylinders   HP   weight\n",
       "0   good          4   75    light\n",
       "1    bad          6   90   medium\n",
       "2    bad          4  110   medium\n",
       "3    bad          8  175  weighty\n",
       "4    bad          6   95   medium\n",
       "5    bad          4   94    light\n",
       "6    bad          4   95    light\n",
       "7    bad          8  139  weighty\n",
       "8    bad          8  190  weighty\n",
       "9    bad          8  145  weighty\n",
       "10   bad          6  100   medium\n",
       "11  good          4   92   medium\n",
       "12   bad          6  100  weighty\n",
       "13   bad          8  170  weighty\n",
       "14  good          4   89   medium\n",
       "15  good          4   65    light\n",
       "16   bad          6   85   medium\n",
       "17  good          4   81    light\n",
       "18   bad          6   95   medium\n",
       "19   bad          4   93    light"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from StringIO import StringIO\n",
    "thefile = StringIO('MPG,cylinders,HP,weight\\ngood,4,75,light\\nbad,6,90,medium\\nbad,4,110,medium\\nbad,8,175,weighty\\nbad,6,95,medium\\nbad,4,94,light\\nbad,4,95,light\\nbad,8,139,weighty\\nbad,8,190,weighty\\nbad,8,145,weighty\\nbad,6,100,medium\\ngood,4,92,medium\\nbad,6,100,weighty\\nbad,8,170,weighty\\ngood,4,89,medium\\ngood,4,65,light\\nbad,6,85,medium\\ngood,4,81,light\\nbad,6,95,medium\\nbad,4,93,light')\n",
    "df = pd.read_csv(thefile)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please use numpy and pandas to do the computation for parts a) through f).  Do not use an existing decision tree implementation like sklearn for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Start with the entire dataset and find the most common MPG value. (2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad is the most common MPG, the value is 15\n"
     ]
    }
   ],
   "source": [
    "goodct = 0\n",
    "badct = 0\n",
    "for i in range(0,len(df)):\n",
    "    if df.iloc[i]['MPG'] == 'good':\n",
    "        goodct += 1\n",
    "    elif df.iloc[i]['MPG'] == 'bad':\n",
    "        badct += 1\n",
    "    else:\n",
    "        print (\"something went wrong with the raw data, please check!\")\n",
    "if goodct > badct:\n",
    "    print (\"good is the most common MPG, the value is %d\" %goodct)\n",
    "else:\n",
    "    print (\"bad is the most common MPG, the value is %d\" %badct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above one gives the common MPG count and also thoroughly examed the data while the below one is precise enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bad     15\n",
       "good     5\n",
       "Name: MPG, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MPG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InformationGain(goodY,badY,goodN,badN):\n",
    "    def F(X,Y):\n",
    "        val1 = X*np.log2(1.*(X+Y)/X) if X>0 else 0\n",
    "        val2 = Y*np.log2(1.*(X+Y)/Y) if Y>0 else 0\n",
    "        return val1+val2\n",
    "    return (F(goodY+goodN,badY+badN)-F(goodY,badY)-F(goodN,badN)) / (goodY+goodN+badY+badN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Enumerate all the possible binary questions you could ask for each discrete-valued variable.  For each such split, compute the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, and compute the information gain using the provided function above. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 0.36529389753193281,\n",
       " 6: 0.15307795338969116,\n",
       " 8: 0.12255624891826571,\n",
       " 'light': 0.097107179451503628,\n",
       " 'medium': 8.8817841970012528e-17,\n",
       " 'weighty': 0.15307795338969116}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "gaindic = {}\n",
    "\n",
    "def binary_node (dataframe,columns):\n",
    "    for name in columns:\n",
    "        ls = dataframe[name].unique().tolist()\n",
    "        for r in ls:\n",
    "            goodY,badY,goodN,badN = 0,0,0,0\n",
    "            for i in range(0,len(df)):\n",
    "                if df.iloc[i][name] == r:\n",
    "                    if df.iloc[i]['MPG'] == 'good':\n",
    "                        goodY +=1\n",
    "                    else: badY += 1\n",
    "                else: \n",
    "                    if df.iloc[i]['MPG'] == 'good':\n",
    "                        goodN +=1\n",
    "                    else: badN += 1\n",
    "            gaindic[r] = InformationGain(goodY,badY,goodN,badN)\n",
    "    return gaindic\n",
    "    \n",
    "gaindic = binary_node(df,['cylinders','weight'])\n",
    "gaindic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Enumerate all the possible binary questions you could ask for the real-valued variable HP.  For each such split, compute the numbers of \"good\" and \"bad\" MPG vehicles in each of the two child nodes, and compute the information gain using the provided function above. (5 pts) \n",
    "\n",
    "NOTE: if you'd like, you can just use all midpoints between consecutive values of the sorted HP attribute.  You are not required to exclude provably suboptimal questions like we did in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the max information gain is', 0.50918592546081209)\n",
      "('the node value is', 92.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-5.0.0-Linux-x86_64/envs/PUI2016_Python2/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "def FindNode(df,column):    \n",
    "    #first, sort the df['HP']\n",
    "    df = df.sort(columns = column)\n",
    "    ls = []\n",
    "    dic = {}\n",
    "    #then get the midpoints between consecutive values\n",
    "    for i in range(len(df)-1):\n",
    "        goodY,badY,goodN,badN = 0,0,0,0\n",
    "        midpoint = (df.iloc[i][column] + df.iloc[i+1][column])/2.0\n",
    "        for m in range(len(df)):\n",
    "            if df.iloc[m][column] > midpoint:\n",
    "                if df.iloc[m]['MPG'] == 'good':\n",
    "                        goodY +=1\n",
    "                else: badY += 1\n",
    "            else:\n",
    "                if df.iloc[m]['MPG'] == 'good':\n",
    "                        goodN +=1\n",
    "                else: badN += 1\n",
    "        ls.append(InformationGain(goodY,badY,goodN,badN))\n",
    "        node_index = ls.index(max(ls))\n",
    "        gain_max = max(ls)\n",
    "        node_value = (df.iloc[node_index][column] + df.iloc[node_index+1][column])/2.0\n",
    "        dic[node_value] = InformationGain(goodY,badY,goodN,badN)\n",
    "    return dic,gain_max,node_value\n",
    "        \n",
    "gaindic_bi, gain_max ,node_value = FindNode(df,'HP')\n",
    "node_notetaker = {}\n",
    "if gain_max > max(gaindic.values()):\n",
    "    print ('the max information gain is', gain_max)\n",
    "    print ('the node value is', node_value)\n",
    "    node_notetaker['HP'] = node_value\n",
    "else: \n",
    "    print max(gaindic)\n",
    "    node_notetaker[max(gaindic)] = max(gaindic.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Based on your results for parts b and c, what is the optimal binary split of the data?  Of the two child nodes created by this split, which (if any) would require further partitioning? (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### first node: HP > 92.5?  The one that HP <= 92.5 need further partitioning (the leaf is not 'pure' enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repeat parts a through d until all training data points are perfectly classified by the resulting tree. (6 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivideDf(df, column, value):\n",
    "    # Divide the rows into two sets and return them\n",
    "    if isinstance(value,int) or isinstance(value,float):\n",
    "        subset1= df.loc[df[column] > value]\n",
    "        subset2= df.loc[df[column] <= value]\n",
    "    else:\n",
    "        subset1= df.loc[df[column] == value] \n",
    "        subset2= df.loc[df[column] != value]\n",
    "    return (subset1,subset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#purity test:\n",
    "def purity_test(set_name):\n",
    "    if set_name['MPG'].value_counts().size == 1:\n",
    "        return ('the leaf is pure')\n",
    "    else: return ('Further partitioning needed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "set1,set2 = DivideDf(df,'HP',node_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the leaf is pure'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity_set1 = purity_test(set1)\n",
    "purity_set1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Further partitioning needed'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity_set2 = purity_test(set2)\n",
    "purity_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Anaconda3-5.0.0-Linux-x86_64/envs/PUI2016_Python2/lib/python2.7/site-packages/ipykernel/__main__.py:3: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "test_columns = ['cylinders','weight']\n",
    "gaindic = binary_node(set1,test_columns)\n",
    "gaindic_bi, node_index,node_value = FindNode(set1,'HP')\n",
    "\n",
    "dic = dict(gaindic.items() + gaindic_bi.items())\n",
    "next_node = max(dic, key=dic.get)\n",
    "\n",
    "for i in test_columns:\n",
    "    if next_node in set1[i].unique().tolist():\n",
    "        column_name = i\n",
    "        break\n",
    "    else: column_name = 'HP'\n",
    "node_notetaker[column_name] = next_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new branches made\n",
      "the tree is built\n",
      "the node list is shown here:\n",
      "{'HP': 92.5, 'cylinders': 4}\n"
     ]
    }
   ],
   "source": [
    "def DivideDf(df, column, value):\n",
    "    # Divide the rows into two sets and return them\n",
    "    if isinstance(value,int) or isinstance(value,float):\n",
    "        subset1= df.loc[df[column] > value]\n",
    "        subset2= df.loc[df[column] <= value]\n",
    "    else:\n",
    "        subset1= df.loc[df[column] == value] \n",
    "        subset2= df.loc[df[column] != value]\n",
    "    return (subset1,subset2)\n",
    "\n",
    "subset1,subset2 = DivideDf(set2,column_name,next_node)\n",
    "print ('new branches made')\n",
    "purity_subset1 = purity_test(subset1)\n",
    "purity_subset2 = purity_test(subset2)\n",
    "\n",
    "if purity_subset1 == 'the leaf is pure' and purity_subset2 == 'the leaf is pure':\n",
    "    print 'the tree is built'\n",
    "elif purity_subset1 != 'the leaf is pure' and purity_subset2 == 'the leaf is pure':\n",
    "    print 'subset1 needs further partitioning!'\n",
    "\n",
    "elif purity_subset1 == 'the leaf is pure' and purity_subset2 != 'the leaf is pure':\n",
    "    print 'subset2 needs further partitioning!'\n",
    "    \n",
    "else: print 'both subsets are not pure!'\n",
    "    \n",
    "print 'the node list is shown here:'\n",
    "print node_notetaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset stat:\n",
      "bad     15\n",
      "good     5\n",
      "Name: MPG, dtype: int64\n",
      "\n",
      "HP > 92.5 \n",
      "bad    13\n",
      "Name: MPG, dtype: int64\n",
      "\n",
      "HP <= 92.5 \n",
      "good    5\n",
      "bad     2\n",
      "Name: MPG, dtype: int64\n",
      "\n",
      "subset_1\n",
      "bad    2\n",
      "Name: MPG, dtype: int64\n",
      "\n",
      "subset_2\n",
      "good    5\n",
      "Name: MPG, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print 'original dataset stat:\\n',df['MPG'].value_counts()\n",
    "print '\\nHP > 92.5 \\n', set1['MPG'].value_counts()\n",
    "print '\\nHP <= 92.5 \\n', set2['MPG'].value_counts()\n",
    "print '\\nsubset_1\\n', subset1['MPG'].value_counts()\n",
    "print '\\nsubset_2\\n', subset2['MPG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MPG</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>HP</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>90</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bad</td>\n",
       "      <td>6</td>\n",
       "      <td>85</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    MPG  cylinders  HP  weight\n",
       "1   bad          6  90  medium\n",
       "16  bad          6  85  medium"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Draw or show the final decision tree in a format of your choice.  The decision to make at each step and the predicted value at each leaf node must be clear. (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            (good 5; bad 15)\n",
    "                                                HP > 92.5?\n",
    "                                                    /\\\n",
    "                                                 Y /  \\ N\n",
    "                                                  /    \\\n",
    "                                    (good 0, bad 13)   (good 5, bad 2)\n",
    "                                          STOP          Cylinders = 4?   \n",
    "                                                             /\\\n",
    "                                                          Y /  \\ N\n",
    "                                                           /    \\\n",
    "                                              (good 5,bad 0)    (good 0, bad 2) \n",
    "                                                 STOP                STOP\n",
    "           Figure 1. The final decision tree "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) Classify each of the following four vehicles as having \"good\" or \"bad\" fuel efficiency (miles per gallon).  Do this by hand using the tree structure learned in part f. (4 pts)\n",
    "\n",
    "bad,8,70,light\n",
    "\n",
    "bad,6,113,medium\n",
    "\n",
    "good,4,83,weighty\n",
    "\n",
    "bad,4,95,weighty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3, Predicting burden of disease （40 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>FrxnPeaceIn10</th>\n",
       "      <th>ODA4H2OPcptaDol</th>\n",
       "      <th>RenewResm3PcptaYr</th>\n",
       "      <th>SustAccImprWatRur</th>\n",
       "      <th>SustAccImprWatUrb</th>\n",
       "      <th>SustAccImprSanRur</th>\n",
       "      <th>SustAccImprSanUrb</th>\n",
       "      <th>TotHlthExpPctofGDP</th>\n",
       "      <th>GenGovtPctofTotHlthExp</th>\n",
       "      <th>ExtResHlthPctTotExpHlth</th>\n",
       "      <th>PCptaGovtExpHlthAvgExcRt</th>\n",
       "      <th>GDPPCptaIntDol</th>\n",
       "      <th>AdultLtrcyRate</th>\n",
       "      <th>FemaleLtrcyRate</th>\n",
       "      <th>BurdenOfDisease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2986</td>\n",
       "      <td>0.10891</td>\n",
       "      <td>0.18812</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.15842</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.4560</td>\n",
       "      <td>4</td>\n",
       "      <td>430</td>\n",
       "      <td>0.35644</td>\n",
       "      <td>0.20792</td>\n",
       "      <td>awful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.58</td>\n",
       "      <td>13306</td>\n",
       "      <td>0.94059</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.801980</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>49</td>\n",
       "      <td>6158</td>\n",
       "      <td>0.85644</td>\n",
       "      <td>0.78713</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.33</td>\n",
       "      <td>473</td>\n",
       "      <td>0.79208</td>\n",
       "      <td>0.91089</td>\n",
       "      <td>0.811880</td>\n",
       "      <td>0.98020</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>71</td>\n",
       "      <td>4860</td>\n",
       "      <td>0.69307</td>\n",
       "      <td>0.60396</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  FrxnPeaceIn10  ODA4H2OPcptaDol  RenewResm3PcptaYr  \\\n",
       "0  Afghanistan            0.1             0.16               2986   \n",
       "1      Albania            1.0             5.58              13306   \n",
       "2      Algeria            0.0             0.33                473   \n",
       "\n",
       "   SustAccImprWatRur  SustAccImprWatUrb  SustAccImprSanRur  SustAccImprSanUrb  \\\n",
       "0            0.10891            0.18812           0.049505            0.15842   \n",
       "1            0.94059            0.98020           0.801980            0.98020   \n",
       "2            0.79208            0.91089           0.811880            0.98020   \n",
       "\n",
       "   TotHlthExpPctofGDP  GenGovtPctofTotHlthExp  ExtResHlthPctTotExpHlth  \\\n",
       "0               0.065                   0.395                   0.4560   \n",
       "1               0.065                   0.417                   0.0340   \n",
       "2               0.041                   0.808                   0.0005   \n",
       "\n",
       "   PCptaGovtExpHlthAvgExcRt  GDPPCptaIntDol  AdultLtrcyRate  FemaleLtrcyRate  \\\n",
       "0                         4             430         0.35644          0.20792   \n",
       "1                        49            6158         0.85644          0.78713   \n",
       "2                        71            4860         0.69307          0.60396   \n",
       "\n",
       "  BurdenOfDisease  \n",
       "0           awful  \n",
       "1             low  \n",
       "2            high  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"Burden of diarrheal illness by country.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dictionary\n",
    "\n",
    "NAME: Burden of diarrheal illness by country\n",
    "\n",
    "SIZE: 130 Countries, 16 Variables\n",
    "\n",
    "VARIABLE DESCRIPTIONS:\n",
    "\n",
    "Country: Country name\n",
    "\n",
    "FrxnPeaceIn10: Fraction of the past ten years in which a country has been at peace \n",
    "\n",
    "ODA4H2OPcptaDol: Per Capita Official Developmental Assistance for water projects\n",
    "\n",
    "RenewResm3PcptaYr: Renewable Water Resources in cubic meters per capita per year\n",
    "\n",
    "SustAccImprWatRur: Fraction of rural population with sustainable access to improved water\n",
    "\n",
    "SustAccImprWatUrb: Fraction of urban population with sustainable access to improved water\n",
    "\n",
    "SustAccImprSanRur: Fraction of rural population with sustainable access to improved sanitation\n",
    "\n",
    "SustAccImprSanUrb: Fraction of urban population with sustainable access to improved sanitation\n",
    "\n",
    "TotHlthExpPctofGDP: Fraction of a country's GDP devoted to health spending\n",
    "\n",
    "GenGovtPctofTotHlthExp: The fraction of total health expenditures for a country which is provided by the government\n",
    "\n",
    "ExtResHlthPctTotExpHlth: The fraction of total health expenditures for a country which is comes from sources external to the country\n",
    "\n",
    "PCptaGovtExpHlthAvgExcRt: Per Capita Government Health Expenditures at the average exchange rate\n",
    "\n",
    "GDPPCptaIntDol: Gross Domestic Product per capita in international dollars\n",
    "\n",
    "AdultLtrcyRate: Adult Literacy rate\n",
    "\n",
    "FemaleLtrcyRate: Female Literacy rate\n",
    "\n",
    "BurdenOfDisease: Our target variable for classification.  The burden of disease due to diarrheal illness, categorized into \"low\", \"medium\", \"high\", and \"awful\" quartiles.  For each country, we have estimates of the number of Disability-Adjusted Life Years lost per 1000 persons per year (DALYs) due to diarrheal illness.  Countries with \"low\" burden of disease have up to 2.75345 DALYs; countries with \"medium\" burden of disease have between 2.75345 and 8.2127 DALYs; countries with \"high\" burden of disease have between 8.2127 and 26.699 DALYs; and countries with \"awful\" burden of diease have more than 26.699 DALYs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your goal is to train a decision tree classifier for the attribute “BurdenOfDisease\" using all other variables (except country name) as features with sklearn.tree.DecisionTreeClassifier. \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Please choose a train/test split and choose a hyper-parameter governing model simplicity, for example, the maximum tree depth or maximum number of leaf nodes. Then, fit your decision tree classifier (using the training set) for different values of this parameter and for each such value, record the corresponding classification accuracy on the test set. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum depth is 1\n",
      "Out of sample accuracy: 0.442307692308\n",
      "\n",
      "\n",
      "maximum depth is 2\n",
      "Out of sample accuracy: 0.596153846154\n",
      "\n",
      "\n",
      "maximum depth is 3\n",
      "Out of sample accuracy: 0.596153846154\n",
      "\n",
      "\n",
      "maximum depth is 4\n",
      "Out of sample accuracy: 0.576923076923\n",
      "\n",
      "\n",
      "maximum depth is 5\n",
      "Out of sample accuracy: 0.615384615385\n",
      "\n",
      "\n",
      "maximum depth is 6\n",
      "Out of sample accuracy: 0.634615384615\n",
      "\n",
      "\n",
      "maximum depth is 7\n",
      "Out of sample accuracy: 0.596153846154\n",
      "\n",
      "\n",
      "maximum depth is 8\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n",
      "maximum depth is 9\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n",
      "maximum depth is 10\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n",
      "maximum depth is 11\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n",
      "maximum depth is 12\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n",
      "maximum depth is 13\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n",
      "maximum depth is 14\n",
      "Out of sample accuracy: 0.653846153846\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# split X and y from the original dataframe:\n",
    "X=np.matrix(data.iloc[:,1:-1])\n",
    "y=np.asarray(data.BurdenOfDisease)\n",
    "\n",
    "# handle the categorical data\n",
    "le = LabelEncoder()\n",
    "le.fit(['low','medium','high','awful'])\n",
    "le.transform(y)\n",
    "\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = 2018)\n",
    "\n",
    "accuracy = []\n",
    "#decision tree classifier\n",
    "for i in range(1,15):\n",
    "    dt = DecisionTreeClassifier(max_depth = i, random_state = 2018)\n",
    "    dt.fit(X_train,y_train)\n",
    "    accuracy.append(dt.score(X_test,y_test))\n",
    "    print 'maximum depth is %d' %i\n",
    "    #print 'In sample accuracy:',dt.score(X_train,y_train)\n",
    "    print 'Out of sample accuracy:',dt.score(X_test,y_test)\n",
    "    print '\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Make a plot of accuracy vs. simplicity for different values of the hyper-parameter chosen in part a). That is, the x-axis should be hyper-parameter value (e.g. tree depth) and the y-axis should be accuracy. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.44230769230769229,\n",
       " 0.59615384615384615,\n",
       " 0.59615384615384615,\n",
       " 0.57692307692307687,\n",
       " 0.61538461538461542,\n",
       " 0.63461538461538458,\n",
       " 0.59615384615384615,\n",
       " 0.65384615384615385,\n",
       " 0.65384615384615385,\n",
       " 0.65384615384615385,\n",
       " 0.65384615384615385,\n",
       " 0.65384615384615385,\n",
       " 0.65384615384615385,\n",
       " 0.65384615384615385]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcMAAAFNCAYAAAB8PAR2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB2paP28dOw4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb622562590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pylab as plt\n",
    "\n",
    "# your code here\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,15,1),accuracy)\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Simplicity (max depth)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2. This is a plot that shows the change of accuracy when the max depth changes. We can see that at the depth of 8, the accuracy reaches its maximum value and does not change anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Tune the hyper-parameter you choose in part a) by cross-validation using the training data. You can choose to use the GridSearchCV package from sklearn or write your own code to do cross-validation by spliting the training data into training and validation data. What is the out of sample accuracy after tuning the hyper-parameter? (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parms:{'max_depth': 2.0}\n",
      " out of sample:0.596153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "thresholds = np.linspace(1,2,50)\n",
    "param_grid = {'max_depth':thresholds}\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state = 2019), param_grid, cv=6)\n",
    "model = gs.fit(X_train,y_train)\n",
    "\n",
    "print(\"best_parms:{0}\\n out of sample:{1}\".format(model.best_params_, model.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Visualize a simple decision tree (e.g., with max_depth = 2 or 3) learned from the data.  To do so, given your decision tree dt, you can use the code below, then copy and paste the resulting output into http://www.webgraphviz.com.  Alternatively, if you have graphviz installed on your machine, you can use that. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digraph Tree {\n",
      "node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=helvetica] ;\n",
      "edge [fontname=helvetica] ;\n",
      "0 [label=\"GDPPCptaIntDol <= 4141.0, samples = 78, value = [19, 23, 19, 17], class = high\", fillcolor=\"#47e53911\"] ;\n",
      "1 [label=\"SustAccImprSanUrb <= 0.644, samples = 38, value = [18, 18, 1, 1], class = awful\", fillcolor=\"#e5813900\"] ;\n",
      "0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"True\"] ;\n",
      "2 [label=\"samples = 24, value = [18, 5, 0, 1], class = awful\", fillcolor=\"#e58139ae\"] ;\n",
      "1 -> 2 ;\n",
      "3 [label=\"samples = 14, value = [0, 13, 1, 0], class = high\", fillcolor=\"#47e539eb\"] ;\n",
      "1 -> 3 ;\n",
      "4 [label=\"GDPPCptaIntDol <= 8727.5, samples = 40, value = [1, 5, 18, 16], class = low\", fillcolor=\"#399de515\"] ;\n",
      "0 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"False\"] ;\n",
      "5 [label=\"samples = 24, value = [0, 5, 4, 15], class = medium\", fillcolor=\"#d739e586\"] ;\n",
      "4 -> 5 ;\n",
      "6 [label=\"samples = 16, value = [1, 0, 14, 1], class = low\", fillcolor=\"#399de5dd\"] ;\n",
      "4 -> 6 ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth = 2, random_state = 2019)\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "thestring=tree.export_graphviz(dt,out_file=None,\n",
    "                         feature_names=data.iloc[:,1:-1].columns.values,  \n",
    "                         class_names=dt.classes_,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False).replace(\"<br/>\",\", \").replace(\"&le;\",\"<=\").replace(\"=<\",\"=\\\"\").replace(\">,\",\"\\\",\")\n",
    "print thestring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![result](./webgraphviz.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4, Fit a random forest to the data from question 3 (20 pts)\n",
    "\n",
    "a) Please use the same test/train split from previous question and feel free to tune the hyper-parameters for Random Forest model using training data. The package from sklearn is here: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.\n",
    "Then please report your out of sample prediction result and compare this model's performance with 3c). (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parms:{'max_depth': 2.0}\n",
      "out of sample accuracy:0.653846153846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "thresholds = np.linspace(1,2,50)\n",
    "param_grid = {'max_depth':thresholds}\n",
    "\n",
    "rfc = GridSearchCV(RandomForestClassifier(max_depth=8,random_state=2018), param_grid, cv=6)\n",
    "gs = rfc.fit(X_train,y_train)\n",
    "\n",
    "print(\"best_parms:{0}\\nout of sample accuracy:{1}\".format(gs.best_params_, gs.score(X_test,y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Write one paragraph comparing the results from those two models (Random Forest vs Decision Tree) in terms of both accuracy and interpretability. (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**As for accuracy, there is a difference between the two methods (random forest performs better, out of sample accuracy = 0.654 while decision tree has 0.596). The difference is not that big might bacause the data in this case is not big/complex enought to show the accuracy difference. From the interpretability side, it is clear that decision tree is a better method to show how the model works (visualizable flow path and much easier to interpret) while it is hard to tell the random forest principles. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "PUI2016_Python2",
   "language": "python",
   "name": "pui2016_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
