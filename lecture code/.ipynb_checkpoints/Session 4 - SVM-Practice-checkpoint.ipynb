{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linearly separable case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1. \"Toy\" example with a straightforward solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from numpy import linalg\n",
    "rcParams['figure.figsize'] = 5, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and visualize the toy data\n",
    "X=np.array([[1,0],[0,1],[-2,0],[0,-2]])\n",
    "Y=np.array([1]*2+[-1]*2)\n",
    "\n",
    "plt.gca()\n",
    "plt.scatter(X[:, 0].T, X[:, 1].T, c=Y, s=300, cmap='spring')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=300, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How to find a line (or more generally, a hyperplane) to separate the data? We could have multiple choices, but which one is the best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gca()\n",
    "plt.scatter(X[:, 0].T, X[:, 1].T, c=Y, s=300, cmap='spring')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=300, cmap='spring')\n",
    "plt.plot([-1.5,1], [1.2,-1.5], 'k-')\n",
    "plt.plot([0.5,-.5],[-2,1], 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to find the hyperplane: $w_1 x_1+w_2 x_2+b=0$ \n",
    "\n",
    "such that:\n",
    "$$\\begin{cases} \\underset{w=(w_1,w_2),b,\\Vert w \\Vert=1}{\\text{max}} M \\\\\n",
    "y_i(x_{i}^{T}w + b)\\ge M, \\: \\forall i\n",
    "\\end{cases}$$\n",
    "\n",
    "With a change of variables, we can get these equivalent conditions for calculation convenience:\n",
    "$$\\begin{cases} \\underset{w,b}{\\text{min}} \\Vert w \\Vert^2 \\\\\n",
    "y_i(x_{i}^{T}w + b)\\ge 1, \\: \\forall i\n",
    "\\end{cases}$$\n",
    "\n",
    "For this simple case, we could solve those conditions by hand, with respect to $X=\\left[(1,0),(0,1),(-2,0),(0,-2)\\right]$ and $Y=(1,1,-1,-1)$:\n",
    "\n",
    "$$\\begin{cases} \\underset{w,b}{\\text{min}} (w_1^2+w_2^2)\\\\\n",
    "w_1+b\\ge1\\hspace{3ex}(E1.1)\\\\\n",
    "w_2+b\\ge1\\hspace{3ex}(E1.2)\\\\\n",
    "(-1)*(-2w_1+b)\\ge1\\hspace{3ex}(E1.3)\\\\\n",
    "(-1)*(-2w_2+b)\\ge1\\hspace{3ex}(E1.4)\n",
    "\\end{cases}$$\n",
    "\n",
    "(E1.1)+(E1.3)$\\Rightarrow$\n",
    "$3 w_1\\ge2$\n",
    "\n",
    "(E1.2)+(E1.4)$\\Rightarrow$\n",
    "$3 w_2\\ge2$\n",
    "\n",
    "(E1.1),(E1.3)$\\Rightarrow$\n",
    "$2 w_1-1\\ge b \\ge 1-w_1$\n",
    "\n",
    "(E1.2),(E1.3)$\\Rightarrow$\n",
    "$2 w_2-1\\ge b \\ge 1-w_2$\n",
    "\n",
    "Then $(w_1^2+w_2^2)$ reaches the minimum under the above conditions for the minimal possible $w_1=w_2=2/3$, and $b=1/3$. Those values satisfy (E1.1)-(E1.4) and provide the definition for the separating line as $2 x_1+2 x_2+1=0$. \n",
    "\n",
    "Then the corresponding maximal value of the margin is $M=\\frac{1}{||w||}=\\frac{1}{\\sqrt{8/9}}=\\frac{3\\sqrt{2}}{4}$.\n",
    "\n",
    "Let's see the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the graph...\n",
    "\n",
    "x1 = np.linspace(-2.5, 2.5)\n",
    "x2 = -x1-0.5 # this is the separating hyperplane that we found\n",
    "M=3*np.sqrt(2)/4 # this is the margin M that we found\n",
    "\n",
    "# plot the parallels to the separating hyperplane that pass through the points\n",
    "x2_down = -x1-0.5+1.5\n",
    "x2_up = -x1-0.5-1.5\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "plt.gca()\n",
    "plt.plot(x1, x2, 'k-')\n",
    "plt.plot(x1, x2_down, 'k--')\n",
    "plt.plot(x1, x2_up, 'k--')\n",
    "plt.axis('tight')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,s=100)\n",
    "plt.arrow(0, 1, -0.25, -1, fc=\"b\", ec=\"b\", head_width=0.07, head_length=0.2)\n",
    "plt.annotate(\"M\", xy=(-0.5, 0.4), fontsize=15)\n",
    "plt.arrow(0, -2, 0.27, 1, fc=\"r\", ec=\"r\", head_width=0.07, head_length=0.2);\n",
    "plt.annotate(\"M\", xy=(0.2, -2), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So in this case all of the points touch the margin and therefore are support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Package.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/svm.html#\n",
    "\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Fit the model.  Using a linear SVM; very large penalty for misclassification.\n",
    "# Since the data is linearly separable, it won't misclassify any points.\n",
    "clf = svm.SVC(kernel='linear',C=10**100)  \n",
    "clf.fit(X, Y)\n",
    "\n",
    "# get the separating hyperplane w[0] x1 + w[1] x2 + intercept = 0\n",
    "# transform to slope-intercept form: x2 = (-w[0]/w[1])x1 - (intercept/w[1])\n",
    "w = clf.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "x1 = np.linspace(-2.5, 2.5)\n",
    "x2 = a * x1 - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "# plot the parallels to the separating hyperplane (slope = a) that go through the support vectors.\n",
    "\n",
    "b = clf.support_vectors_[0]\n",
    "x2_down = a * x1 + (b[1] - a * b[0])\n",
    "\n",
    "b = clf.support_vectors_[-1]\n",
    "x2_up = a * x1 + (b[1] - a * b[0])\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "plt.gca()\n",
    "plt.plot(x1, x2, 'k-')\n",
    "plt.plot(x1, x2_down, 'k--')\n",
    "plt.plot(x1, x2_up, 'k--')\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=200, facecolors='none')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired,s=100)\n",
    "\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Soft Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data that are _not_ fully separable this time.\n",
    "np.random.seed(999)\n",
    "mean1 = np.array([0, 2])\n",
    "mean2 = np.array([2, 0])\n",
    "cov = np.array([[1.8, 1.0], [1.0, 1.8]])\n",
    "X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "y1 = np.ones(len(X1))\n",
    "X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "y2 = np.ones(len(X2)) * -1\n",
    "\n",
    "plt.gca()\n",
    "plt.scatter(X1[:,0],X1[:,1], c='r', cmap=plt.cm.Paired)\n",
    "plt.scatter(X2[:,0],X2[:,1], c='b', cmap=plt.cm.Paired)\n",
    "X=np.concatenate((X1,X2),axis=0)\n",
    "Y=np.concatenate((y1,y2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, we cannot perfectly separate these two clusters with a line.\n",
    "# let's use a soft margin classifier model over the entire data with C=1.\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1) # You can try other C; for this example the model is not too sensitive to choice of C.\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot the line, the points, and the nearest vectors to the plane.\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            facecolors='none', zorder=10) # plot support vectors with small circle\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired) # plot X,Y\n",
    "\n",
    "\n",
    "###########################################################################################\n",
    "plt.axis('tight')\n",
    "x_min = -6\n",
    "x_max = 8\n",
    "y_min = -6\n",
    "y_max = 8\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] # all the points in the plane\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) # put them in the decision function\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired) # Make a color for all the points in plane by our decision function.\n",
    "\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-.5, 0, .5])\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "# Let's calculate the IS errors: (Just use clf.predict and compare the predicted labels with true labels)\n",
    "print \"In sample, we successfully predict {} percent of the data\".format(100-abs(clf.predict(X)-Y).sum()*50/len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the out-of-sample performance.\n",
    "# As usual, let's divide our data into training and test data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for i in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=i)\n",
    "\n",
    "    clf = svm.SVC(kernel='linear', C=1) \n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    correct=1.0*(clf.predict(X_test)==np.asarray(Y_test)).sum()/len(Y_test)\n",
    "\n",
    "    print \"Out of sample, we successfully predict {} percent of the data\".format((correct)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice #1:  \n",
    "\n",
    "Let C=np.exp(-10), and report the average out-of-sample accuracy over 10 random train/test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Practice #2: Optimal C tuning.\n",
    "\n",
    "a. Try C in [np.exp(i) for i in np.linspace(-5,5,100)] to find the optimal C using the training set.\n",
    "Use the package GridSearchCV from sklearn.model_selection.\n",
    "\n",
    "b. Test your model using the testing set(Just fix your C = optimal C you found).\n",
    "\n",
    "Average your results from parts a and b over 10 different random train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reminder:\n",
    "print range(-5,5) # integers; does not include endpoints\n",
    "print np.linspace(-5,5,100) # reals; does include endpoints by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### For one random train/test split, let's plot the OS prediction accuracy as a function of C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33,random_state=70)\n",
    "\n",
    "# When C is very small, we are willing to tolerate more mistakes. If C is very big, this\n",
    "# means we hardly tolerate any mistakes. So, we cannot choose a very large C if our data is not\n",
    "# really separable. Let's however choose from a broad range of reasonable options.\n",
    "C = [np.exp(i) for i in np.linspace(-10,10,300)]\n",
    "OS = []\n",
    "for c in C:\n",
    "    clf = svm.SVC(kernel='linear',C=c) \n",
    "    clf.fit(X_train, Y_train)\n",
    "    correct=1.0*(clf.predict(X_test)==np.asarray(Y_test)).sum()/len(Y_test)\n",
    "    OS.append(correct)\n",
    "\n",
    "plt.gca()\n",
    "plt.plot(np.linspace(-10,10,300),OS)\n",
    "plt.xlabel(\"log C\")\n",
    "plt.ylabel(\"OS accuracy\")\n",
    "plt.title(\"Accuracy vs. penalization constant (log C)\")\n",
    "plt.xlim(-10,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-linearly separable case: Gaussian and polynomial kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using Kernels (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this data we could use SVM with a Gaussian kernel to separate\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "X, Y = make_circles(500, factor=.1, noise=.1,random_state=999)\n",
    "\n",
    "plt.gca()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a quick look at Linear SVM\n",
    "clf = svm.SVC(kernel='linear',C=1) \n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot the line, the points, and the nearest vectors to the plane\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            facecolors='none', zorder=10) # plot support vectors with small circle\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired) # plot X,Y\n",
    "\n",
    "plt.axis('tight')\n",
    "x_min = -1.5\n",
    "x_max = 1.5\n",
    "y_min = -1.5\n",
    "y_max = 1.5\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] # all the points in the plane\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) # put them in the decision function\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired) # Make a color for all the points in plane by our decision function.\n",
    "\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-.5, 0, .5])\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "# Let's calculate the IS mistakes: (Just use clf.predic to compare the predicted labels with current labels)\n",
    "print \"In sample, we successfully predict {} percent of the data\".format(1.0*(clf.predict(X)==Y).sum()/len(Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Kernel SVM, aka \"Radial Basis Function\" or \"RBF\".\n",
    "# Gamma is an extra bandwidth parameter\n",
    "clf = svm.SVC(kernel='rbf',gamma=1) \n",
    "\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot the line, the points, and the nearest vectors to the plane\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "            facecolors='none', zorder=10) # plot support vectors with small circle\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired) # plot X,Y\n",
    "\n",
    "\n",
    "#####################################################################################\n",
    "plt.axis('tight')\n",
    "x_min = -1.5\n",
    "x_max = 1.5\n",
    "y_min = -1.5\n",
    "y_max = 1.5\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j] # all the points in the plane\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) # put them in the desion function\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired) # Make a color for all the points in plane by our decision function.\n",
    "\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-.5, 0, .5])\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print \"In sample, we successfully predict {} percent of the data\".format(100.0*(clf.predict(X)==Y).sum()/len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4. Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(0)\n",
    "X = np.random.randn(500, 2)\n",
    "Y = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)*1\n",
    "plt.gca()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=50, cmap='spring')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obviously, a linear model won't work.\n",
    "# Let's try a soft margin linear classifier model on the entire data with C=1. \n",
    "# You can try other C as well, but the visual intuition hints that it won't help much.\n",
    "\n",
    "clf = svm.SVC(kernel='linear',C=1) \n",
    "clf.fit(X, Y)\n",
    "\n",
    "print \"In sample, we successfully predict {} percent of the data\".format((clf.predict(X)==Y).sum()*100/len(Y))\n",
    "print \"This result means that we cannot get any useful information. So we should consider non-linear kernels.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use polynomial kernel with degree 2 \n",
    "clf = svm.SVC(kernel='poly',degree=2) \n",
    "clf.fit(X, Y)\n",
    "\n",
    "# Plot the line, the points, and the nearest vectors to the plane\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,\n",
    "            facecolors='none', zorder=10)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, zorder=10, cmap=plt.cm.Paired)\n",
    "\n",
    "\n",
    "################################################################################\n",
    "plt.axis('tight')\n",
    "x_min = -3\n",
    "x_max = 3\n",
    "y_min = -3\n",
    "y_max = 3\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]\n",
    "Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'],\n",
    "            levels=[-.5, 0, .5])\n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()\n",
    "\n",
    "print \"In sample, we successfully predict {} percent of the data\".format((Y==clf.predict(X)).sum()*100/len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how we do out of sample, dividing the dataset into training and test, and averaging over 10 random splits.\n",
    "from sklearn.model_selection import train_test_split\n",
    "OS = []\n",
    "for i in range(10):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=i)\n",
    "\n",
    "    clf = svm.SVC(kernel='poly',degree=2) \n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    correct=1.0*(clf.predict(X_test)==Y_test).sum()/len(Y_test)\n",
    "    OS.append(correct)\n",
    "    \n",
    "print \"Out of sample, we successfully predict {} percent of the data\".format((np.mean(correct))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of individual/commercial houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"NYC_individual_commercial.csv\")\n",
    "print data.head()\n",
    "X=data.iloc[:,:4]\n",
    "Y=data.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization highly recommended prior to using SVM.  \n",
    "# You should scale all real-valued attributes and replace any discrete-valued attributes with dummy variables.\n",
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)\n",
    "X=X_scaled\n",
    "print pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear model with soft margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first try a linear model with parameter value C=1.\n",
    "clf = svm.SVC(kernel='linear',C=1)\n",
    "clf.fit(X_train, Y_train)\n",
    "correct=1.0*(clf.predict(X_test)==np.asarray(Y_test)).sum()/len(Y_test)\n",
    "print \"Out of sample, linear model successfully predicts {} percent of the data\".format((correct)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do cross validation for choosing C since we see that the accuracy is not so good.\n",
    "# We could have used GridSearchCV, but let's try optimizing the parameter manually\n",
    "\n",
    "# Divide the training set into training set and validation set.\n",
    "X_train_1,X_vali,Y_train_1,Y_vali = train_test_split(X_train, Y_train, test_size=0.33, random_state=2999)\n",
    "\n",
    "C = [np.exp(i) for i in np.linspace(-8,8,50)] \n",
    "OS_validation=[]\n",
    "for c in C:\n",
    "    clf = svm.SVC(kernel='linear',C=c) \n",
    "    clf.fit(X_train_1, Y_train_1)\n",
    "    correct=1.0*(clf.predict(X_vali)==np.asarray(Y_vali)).sum()/len(Y_vali) # OS score for validation set\n",
    "    OS_validation.append(correct)\n",
    "    \n",
    "temp=pd.DataFrame([C,OS_validation]).T # put results together.\n",
    "print temp\n",
    "\n",
    "C=[np.log(y) for y in C] # for a better graph\n",
    "plt.gca()\n",
    "plt.plot(C,OS_validation,'b',)\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('log(C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We choose C based on the validation results above, and see the out-of-sample accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel='linear',C=500)\n",
    "clf.fit(X_train, Y_train)\n",
    "correct=1.0*(clf.predict(X_test)==np.asarray(Y_test)).sum()/len(Y_test)\n",
    "print \"Out of sample, we successfully predict {} percent of the data using a linear kernel\".format((correct)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice #3: Polynomial kernel\n",
    "\n",
    "Use the default hyper-parameters (C=1, Poly=3) to train the model using training data. Please report the OS accuracy on your testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Practice #4: Parameter tuning\n",
    "\n",
    "Please use the training set to pick the optimal model from the following combinations of options, using GridSearchCV:\n",
    "\n",
    "degree in range(1,7)\n",
    "\n",
    "C in 10**np.linspace(-5,10,20)\n",
    "\n",
    "coef0 in 10**np.linspace(-3,3,20)\n",
    "\n",
    "max_iter=100\n",
    "\n",
    "What is the out-of-sample accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice #5. Repeat Practice #4 using a Gaussian Kernel. \n",
    "\n",
    "Please feel free to check any possible hyperparameters' combinations you would like but only use the training data. (You need to check C and gamma at least. But other hyperparameters related to the training process could be changed as well).\n",
    "\n",
    "#### Competition!\n",
    "Report your OS accuracy on the given testing data, and let's see who gets the best result. \n",
    "Note: it could depend on luck, since it is only one particular training/test split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
