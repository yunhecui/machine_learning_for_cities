{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection and Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Cross-validation: evaluating estimator performance\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "#### (2) Or it could be used for tuning hyper-parameters.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/grid_search.html#grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating estimator performance.\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing supervised machine learning to hold out part of the available data as a test set (X_test, y_test), then to learn a model on the remaining (training) data and evaluate its accuracy on the test data.\n",
    "\n",
    "#### Tuning hyper-parameters\n",
    "If you have a model with important hyper-parameters (e.g., the amount of penalization for Lasso or Ridge regression), you can tune (find good values of) these hyperparameters by further splitting the training data into a training and validation set (still keeping the test data separate from these for a final, unbiased measure of performance).  To tune the hyper-parameters you can try a range of parameter values, learning from the (reduced) training set and evaluating performance on the validation set, and choose the hyper-parameters with best validation set performance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data: predicting housing price using 311 calls for service\n",
    "path = 'https://serv.cusp.nyu.edu/~cq299/ADS2016/Data/Bayesian/'\n",
    "data4=pd.read_csv(path + \"example4.csv\", low_memory=False)\n",
    "list_311=list(data4.loc[:,\"Adopt A Basket\":\"X Ray Machine Equipment\"].columns)\n",
    "data5=data4[[\"sale_price\",\"gross_sq_feet\",\"mean\"]+list_311]\n",
    "data5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.matrix(data5.iloc[:,1:])\n",
    "y=np.asarray(data5.sale_price)\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-sample R^2 value for ridge regression using the whole training dataset\n",
    "from sklearn import linear_model\n",
    "lm=linear_model.LinearRegression()\n",
    "lm.fit(X,y)\n",
    "1-((lm.predict(X)-y)**2).mean()/y.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well do we do out of sample?  Let's split the data into 60% training, 40% test, and average performance over 10 random splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "OS=[]\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = i)    \n",
    "    lm=linear_model.LinearRegression()\n",
    "    lm.fit(X_train,y_train)\n",
    "    OS.append(1-((lm.predict(X_test)-y_test)**2).mean()/y_test.var()) # or equivalently: OS.append(lm.score(X_test,y_test))\n",
    "print np.mean(OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does ridge regression do better out of sample?  Let's try with an arbitrary choice of penalization parameter alpha = 1.\n",
    "OS=[]\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = i)    \n",
    "    lm=linear_model.Ridge(alpha=1)\n",
    "    lm.fit(X_train,y_train)\n",
    "    OS.append(1-((lm.predict(X_test)-y_test)**2).mean()/y_test.var()) # or equivalently: OS.append(lm.score(X_test,y_test))\n",
    "print np.mean(OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n",
    "\n",
    "Exhaustive Grid Search. The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's tune the hyper-parameters for ridge regression and calculate the OS R-squared using the new tuned alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid ={'alpha':np.logspace(-4, 0, 200)}\n",
    "\n",
    "OS=[]\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state = i)\n",
    "    rid=linear_model.Ridge()\n",
    "    gr=GridSearchCV(rid,param_grid=param_grid)\n",
    "    rs=gr.fit(X_train,y_train)\n",
    "    print rs.best_params_\n",
    "    OS.append(1-((rs.predict(X_test)-y_test)**2).mean()/y_test.var())\n",
    "print np.mean(OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Decision Tree. \n",
    "\n",
    "Stop-and-Frisk Data set: https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please download the data set here or on NYU-Classes.\n",
    "https://serv.cusp.nyu.edu/classes/ML_2016_Spring/ML_2017/\n",
    "\n",
    "file: \"session_3_stop.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"session_3_stop.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records with any missing values\n",
    "data=data.dropna()\n",
    "\n",
    "# Let's take \"found.weapon\" as the target variable. \n",
    "y=data.loc[:,\"found.weapon\"]\n",
    "\n",
    "# Get the feature space.  We are using only features from before the stop, getting rid of features from during/after the stop like \"arrested\".\n",
    "X=data.loc[:,\"suspect.race\":\"time.period\"]\n",
    "X=pd.get_dummies(X)\n",
    "\n",
    "# Split data into 70% train, 30% test\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=999)\n",
    "print X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part one: IS and OS Accuracy of prediction. \n",
    "\n",
    "DecisionTreeClassifier:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# learn model\n",
    "dt=DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# in sample accuracy\n",
    "print 'In sample accuracy:',dt.score(X_train,y_train)\n",
    "\n",
    "# out of sample accuracy\n",
    "print 'Out of sample accuracy:',dt.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #1: Get the average OS accuracy over 10 train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does that mean our model is super good? However... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would our accuracy be if we predicted 0 (no weapon found) for everyone?\n",
    "print 1.*len(y_test[y_test==0])/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use area under the receiver operating characteristic curve (\"ROC AUC\") instead of accuracy.\n",
    "\n",
    "ROC, Area under the curve:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics\n",
    "\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic\n",
    "\n",
    "Youtube:\n",
    "https://www.youtube.com/watch?v=hnRBl9-BzjQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "AUC_OS=[]\n",
    "for i in range(10):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    dt=DecisionTreeClassifier()\n",
    "    dt.fit(X_train,y_train)\n",
    "    # predict_proba predicts the probability of each class rather than just the most likely class\n",
    "    pred=dt.predict_proba(X_test)[:,1] # predicted probability of y = 1\n",
    "    AUC_OS.append(roc_auc_score(np.array(y_test),pred))\n",
    "print \"OS AUC\",np.mean(AUC_OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How well would we expect to do just by chance?\n",
    "chance_OS=[]\n",
    "for i in range(10):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3, random_state=i)\n",
    "    pred=np.random.random(len(X_test))\n",
    "    chance_OS.append(roc_auc_score(np.array(y_test.apply(int)),pred))\n",
    "print np.mean(chance_OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control the complexity of the Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just use a single train/test split for this part:\n",
    "X_train,X_test,y_train,y_test=train_test_split(X, y, test_size=0.3,random_state=999)\n",
    "AUC_OS=[]\n",
    "for i in range(2,500,25):\n",
    "    dt=DecisionTreeClassifier(max_leaf_nodes=i)\n",
    "    dt.fit(X_train,y_train)\n",
    "    AUC_OS.append(roc_auc_score(np.array(y_test),dt.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(2,500,25),AUC_OS)\n",
    "plt.xlabel(\"Max leaf nodes\")\n",
    "plt.ylabel(\"OS_AUC\")\n",
    "plt.title(\"AUC vs Simplicity (max leaf nodes)\")\n",
    "plt.xlim(2,500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an aside: predict_proba vs. predict\n",
    "tm=pd.concat((pd.DataFrame(dt.predict_proba(X_test)),pd.DataFrame(dt.predict(X_test))),axis=1)\n",
    "tm.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we'll use max_depth to control the complexity of the tree, still using the same train/test split as above,\n",
    "# and optimize the parameter value using GridSearchCV.\n",
    "param_grid = {'max_depth':range(1,11)}\n",
    "dt=DecisionTreeClassifier()\n",
    "gr=GridSearchCV(dt,param_grid=param_grid,scoring='roc_auc')\n",
    "rs=gr.fit(X_train,y_train)\n",
    "print rs.best_params_\n",
    "print roc_auc_score(np.array(y_test),rs.predict_proba(X_test)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "Decision trees can be used for feature selection by calculating the Gini importance of each feature (higher = more important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=8)\n",
    "dt.fit(X_train, y_train)\n",
    "Feature_importance=pd.DataFrame([list(X_train.columns),list(dt.feature_importances_)]).T\n",
    "Feature_importance.columns=[\"variables\",\"importance\"]\n",
    "\n",
    "# list the top 5 most important features in order\n",
    "Feature_importance.sort_values(by=\"importance\",ascending=False).iloc[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate our new training and testing model using the top three features.\n",
    "X_train_simple=X_train.loc[:,[\"stopped.bc.object\",\"location.housing_transit\",\"precinct\"]]\n",
    "X_test_simple=X_test.loc[:,[\"stopped.bc.object\",\"location.housing_transit\",\"precinct\"]]\n",
    "\n",
    "# Now let's see the performance of this simple model.\n",
    "dt = DecisionTreeClassifier(max_depth=8)\n",
    "dt.fit(X_train_simple, y_train)\n",
    "print \"The AUC score for this simple model with 3 features is\",roc_auc_score(y_test,dt.predict_proba(X_test_simple)[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Tree we built. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2) # just to keep it simple for visualization\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "# display the output using www.webgraphviz.com, or if you have GraphViz installed on\n",
    "# your computer, you can use that\n",
    "print tree.export_graphviz(dt,out_file=None,\n",
    "                         feature_names=X_train.columns.values,  \n",
    "                         class_names=['no weapon found','weapon found'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False).replace(\"<br/>\",\", \").replace(\"&le;\",\"<=\").replace(\"=<\",\"=\\\"\").replace(\">,\",\"\\\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to install GraphViz on your own machine:\n",
    "\n",
    "conda install graphviz\n",
    "\n",
    "pip install pydot\n",
    "\n",
    "pip install pydotplus\n",
    "\n",
    "For people who experienced this error: \"GraphViz's executables not found\"\n",
    "\n",
    "http://stackoverflow.com/questions/18438997/why-is-pydot-unable-to-find-graphvizs-executables-in-windows-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only work if GraphViz is installed on your machine\n",
    "from sklearn import tree\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "thestring = tree.export_graphviz(dt, out_file=None,  \n",
    "                         feature_names=X_train.columns.values, \n",
    "                         class_names=['no weapon found','weapon found'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True,impurity=False)\n",
    "graph = pydotplus.graph_from_dot_data(thestring)  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same training data as above\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=30, n_jobs=-1,max_leaf_nodes=10)\n",
    "rf.fit(X_train, y_train)\n",
    "pred=rf.predict_proba(X_test)[:,1]\n",
    "print roc_auc_score(y_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #2. Let's fix max_leaf_nodes=10, build forests with between 1 and 50 trees, and plot the AUC as a function of number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #3.  Use GridSearchCV to optimize the hyperparameter, num_estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the result (time permitting)\n",
    "\n",
    "For this part, let's select two categorical features, and make two corresponding plots\n",
    "comparing the average model prediction and empirical outcome for\n",
    "each value of that feature. \n",
    "\n",
    "For example, if a chosen feature is\n",
    "‘precinct’, the plot should have a point for each precinct, where the\n",
    "x-value is the average model prediction for all stops in that precinct,\n",
    "and the y-value is the average value of your target variable for all\n",
    "stops in that precinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=50, n_jobs=4,max_leaf_nodes=30)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Let's pick two features, age and precinct, and save them with the result\n",
    "result=X_test.loc[:,['suspect.age','precinct']]\n",
    "result=pd.concat((y_test,result),axis=1)\n",
    "result['pred_prob']=rf.predict_proba(X_test)[:,1]\n",
    "result.index=range(len(result))\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=result.groupby(\"precinct\").apply(lambda x: x.loc[:,[\"found.weapon\",\"pred_prob\"]].mean()).reset_index()\n",
    "aa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For precinct:\n",
    "plt.figure(figsize=(8,7.5))\n",
    "plt.scatter(list(aa.loc[:,\"pred_prob\"]),list(aa.loc[:,\"found.weapon\"]),c=\"r\")\n",
    "plt.title('Predicted weapon probability vs Real weapon probability by \"Precinct\"')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Real\")\n",
    "plt.xlim(0,0.5)\n",
    "plt.ylim(-0.01,0.5)\n",
    "plt.plot(list(aa.loc[:,\"found.weapon\"]),list(aa.loc[:,\"found.weapon\"]),'-b')\n",
    "\n",
    "#Let's label the precincts for which our model does not work well.\n",
    "for label, x, y in zip(list(aa.precinct), list(aa.loc[:,\"pred_prob\"]), list(aa.loc[:,\"found.weapon\"])):\n",
    "    if x-y>0.05:\n",
    "        \n",
    "        plt.annotate(\n",
    "            label, \n",
    "            xy = (x, y), xytext = (30, -30),\n",
    "            textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "            bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "            arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "    if x-y<-0.05:\n",
    "        plt.annotate(\n",
    "            label, \n",
    "            xy = (x, y), xytext = (-20, 20),\n",
    "            textcoords = 'offset points', ha = 'right', va = 'bottom',\n",
    "            bbox = dict(boxstyle = 'round,pad=0.5', fc = 'yellow', alpha = 0.5),\n",
    "            arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice #4\n",
    "\n",
    "Repeat this process and plot the result for \"suspect.age\" or for any feature you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
