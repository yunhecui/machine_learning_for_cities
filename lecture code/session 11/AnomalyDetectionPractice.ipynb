{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based anomaly detection.\n",
    "\n",
    "#### Given a learned Bayes Net structure, the lowest-likelihood data records are considered most anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"NYC_taxi_sample.csv\")\n",
    "\n",
    "# make all columns small integer counts (0, 1, ..., cardinality-1)\n",
    "data.loc[:,'tip':'pass'] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Learn structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "from pgmpy.estimators import BicScore\n",
    "hc = HillClimbSearch(data, scoring_method=BicScore(data))\n",
    "best_model = hc.estimate()\n",
    "print(best_model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Learn parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "\n",
    "model = BayesianModel(best_model.edges())\n",
    "model.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "for cpd in model.get_cpds():\n",
    "    print(\"CPD of {variable}:\".format(variable=cpd.variable))\n",
    "    print(cpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Compute log-likelihood of each data record given the model, and report the lowest likelihood (most anomalous) records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-likelihood of data record x given model\n",
    "def LL(x,model,verbose=False):\n",
    "    loglike = 0\n",
    "    for cpd in model.get_cpds():\n",
    "        temp_cpd = cpd.copy()\n",
    "        thevariable = temp_cpd.variable\n",
    "        theparents = model.predecessors(thevariable)\n",
    "        for parent in theparents:\n",
    "            temp_cpd.reduce([(parent, x[parent])])\n",
    "        theprob = temp_cpd.get_values()[x[thevariable],0]\n",
    "        if verbose:\n",
    "            print thevariable,theparents,theprob\n",
    "        loglike += np.log(theprob)\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute anomalousness of each of the first 500 data records\n",
    "exmp=data.iloc[:500,:].apply(lambda x: LL(x,model),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.scatter(range(len(exmp)),exmp)\n",
    "plt.xlim(-10,520)\n",
    "plt.plot(exmp,\"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 most anomalous data records\n",
    "print exmp.sort_values().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the most anomalous record\n",
    "print data.iloc[392,:]\n",
    "print\n",
    "LL(data.iloc[392,:],model,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the second most anomalous record\n",
    "print data.iloc[380,:]\n",
    "print\n",
    "LL(data.iloc[380,:],model,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster-based anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cluster with Gaussian Mixture.  Look for records with low log-likelihood as well as any tiny clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example with Fisher's iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:100]\n",
    "y = iris.target[:100] # not used- unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "GM=GaussianMixture(n_components=3,random_state=999)\n",
    "GM.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_samples function gives the log of the probability density of each data record given its cluster.\n",
    "# Note that probability densities can exceed 1 (unlike probabilities of discrete data).\n",
    "import matplotlib.pylab as plt\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.scatter(range(len(X)),GM.score_samples(X))\n",
    "plt.plot(GM.score_samples(X),\"r--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 most anomalous data points by log-likelihood (i.e. log probability density)\n",
    "pd.DataFrame(GM.score_samples(X)).sort_values(0)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also check for any very tiny clusters\n",
    "print pd.Series(GM.predict(X)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Cluster with k-means.  Look for records with large distance to the nearest cluster center as well as any tiny clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "KM=KMeans(n_clusters=3,random_state=999)\n",
    "scor=KM.fit_predict(X)\n",
    "\n",
    "# distance to each cluster center\n",
    "res=pd.DataFrame(KM.transform(X))\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster and distance information for each data record\n",
    "res=pd.DataFrame(KM.transform(X))\n",
    "res=pd.concat((res,pd.DataFrame(KM.fit_predict(X))),axis=1)\n",
    "res.columns=list(range(3))+[\"cluster\"]\n",
    "res.loc[:,\"score\"]=res.apply(lambda x: x[int(x[\"cluster\"])],axis=1)\n",
    "\n",
    "# find data records farthest from cluster centers\n",
    "res.sort_values(\"score\",ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for tiny clusters\n",
    "print res['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Question\n",
    "\n",
    "The data we will use here are the hourly bicycle counts on Seattle's Fremont Bridge. These data come from an automated bicycle counter, installed in late 2012, which has inductive sensors under the sidewalks on either side of the bridge.  Our goal is to detect days with abnormal counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(\"Bridge.csv\",index_col=\"Date\",parse_dates=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the data\n",
    "data.resample('W').sum().plot(figsize=(20,5))\n",
    "plt.ylabel('weekly trips')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts for (West,East) for each hour of each day\n",
    "pivoted = data.pivot_table(['East', 'West'],\n",
    "                           index=data.index.date,\n",
    "                           columns=data.index.hour,\n",
    "                           fill_value=0)\n",
    "days=pivoted.index\n",
    "X=pivoted.values\n",
    "print pivoted.head()\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1. Detect abnormal days using Gaussian mixture clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data so that the 48 columns for a given day sum to 1.  You can interpret each value as\n",
    "# the proportion of that day's trips that are in a given direction (west or east) on a given hour.\n",
    "X = pivoted.values\n",
    "X=X/X.sum(1).reshape(-1,1)\n",
    "data2=pd.concat((pd.DataFrame(days),pd.DataFrame(X)),axis=1)\n",
    "data2.columns=[\"date\"]+list(data2.columns)[1:]\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now cluster the data using 5 Gaussian mixture components, and identify the 5 most anomalous days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Detect abnormal days using k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data so that the 48 columns for a given day sum to 1.  You can interpret each value as\n",
    "# the proportion of that day's trips that are in a given direction (west or east) on a given hour.\n",
    "X = pivoted.values\n",
    "X=X/X.sum(1).reshape(-1,1)\n",
    "data3=pd.concat((pd.DataFrame(days),pd.DataFrame(X)),axis=1)\n",
    "data3.columns=[\"date\"]+list(data3.columns)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now cluster the data using k-means clustering with k=5, and identify the 5 most anomalous days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.  Detect anomalous days using Bayesian networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's reduce from 48 variables to 10: proportions for midnight-1am, 11am-noon, 3-4pm, 6-7pm, 9-10pm going in each direction\n",
    "X = pivoted.values\n",
    "X=X/X.sum(1).reshape(-1,1)\n",
    "XX = XX=X[:,[0,11,15,18,21,24,35,39,42,45]]\n",
    "data4=pd.concat((pd.DataFrame(days),pd.DataFrame(XX)),axis=1)\n",
    "data4.columns=[\"date\",\"W0\",\"W11\",\"W15\",\"W18\",\"W21\",\"E0\",\"E11\",\"E15\",\"E18\",\"E21\"]\n",
    "\n",
    "# discretize each variable to four equal-frequency bins (quartiles)\n",
    "for i in data4.iloc[:,1:]:\n",
    "    data4[i] = pd.qcut(data4[i],q=4,labels=False)\n",
    "print data4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other anomaly detection methods in sklearn (time permitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOF\n",
    "\n",
    "Local outlier factor (LOF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Generate train data\n",
    "np.random.seed(42)\n",
    "X = 0.3 * np.random.randn(100, 2)\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "X = np.r_[X + 2, X - 2, X_outliers]\n",
    "\n",
    "# fit the model\n",
    "clf = LocalOutlierFactor(n_neighbors=20)\n",
    "y_pred = clf.fit_predict(X)\n",
    "y_pred_outliers = y_pred[200:]\n",
    "\n",
    "# plot the level sets of the decision function\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "Z = clf._decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "\n",
    "plt.title(\"Local Outlier Factor (LOF)\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "a = plt.scatter(X[:200, 0], X[:200, 1], c='white')\n",
    "b = plt.scatter(X[200:, 0], X[200:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a, b],\n",
    "           [\"normal observations\",\n",
    "            \"abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Class SVM and Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 500), np.linspace(-5, 5, 500))\n",
    "# Generate train data\n",
    "X = 0.3 * np.random.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * np.random.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.1)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "n_error_train = y_pred_train[y_pred_train == -1].size\n",
    "n_error_test = y_pred_test[y_pred_test == -1].size\n",
    "n_error_outliers = y_pred_outliers[y_pred_outliers == 1].size\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Novelty Detection\")\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), 0, 7), cmap=plt.cm.PuBu)\n",
    "a = plt.contour(xx, yy, Z, levels=[0], linewidths=2, colors='darkred')\n",
    "plt.contourf(xx, yy, Z, levels=[0, Z.max()], colors='palevioletred')\n",
    "\n",
    "s = 40\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white', s=s)\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='blueviolet', s=s)\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='gold', s=s)\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([a.collections[0], b1, b2, c],\n",
    "           [\"learned frontier\", \"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\",\n",
    "           prop=matplotlib.font_manager.FontProperties(size=11))\n",
    "plt.xlabel(\n",
    "    \"error train: %d/200 ; errors novel regular: %d/40 ; \"\n",
    "    \"errors novel abnormal: %d/40\"\n",
    "    % (n_error_train, n_error_test, n_error_outliers))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate train data\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X_train = np.r_[X + 2, X - 2]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * rng.randn(20, 2)\n",
    "X_test = np.r_[X + 2, X - 2]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = rng.uniform(low=-4, high=4, size=(20, 2))\n",
    "\n",
    "# fit the model\n",
    "clf = IsolationForest(max_samples=100, random_state=rng)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "\n",
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "xx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-5, 5))\n",
    "plt.ylim((-5, 5))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn makes a distinction between \"novelty detection\" with clean training data and \"outlier detection\" with training data possibly corrupted by a small proportion of anomalies.  This is _not_ standard usage-- anomaly, outlier, and novelty detection are generally used interchangeably in the literature, and it should be separately specified whether or not the training data is assumed to be clean."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
